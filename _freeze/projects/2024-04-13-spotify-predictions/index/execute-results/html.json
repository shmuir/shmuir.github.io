{
  "hash": "7152e2e80d3f0d1e2ea54b29fae46238",
  "result": {
    "markdown": "---\ntitle: \"Who's on AUX? Using machine learning to predict whose Spotify playlist a song belongs to\"\ndescription: \"\"\nauthor:\n  - name: Sam Muir\n    url: https://shmuir.github.io/\n  #  orcid: \n    affiliation: Bren School of Environmental Science & Management at University of California Santa Barbara\n    affiliation-url: https://bren.ucsb.edu/masters-programs/master-environmental-data-science\ndate: 04-13-2024\ncategories: [ML, R, MEDS]\nbibliography: references.bib\ncitation:\n  url: https://shmuir.github.io/projects/2024-04-13-spotify-predictions/\nimage: trees.jpeg\nformat:\n  html:\n    code-fold: false\ndraft: false\n---\n\n\n\n\n\n\n# Project Overview\n\n## Motivation\n\nIn this project, I am interested in building my machine learning skills through a fun exercise with Spotify! \n\n## Question\n\nUsing my liked songs and my friends, can I build a machine learning model to predict whose playlist a song belongs to? \n\n# Data & Methods\n\nFor this project, I will be requesting data from the Spotify API using the {spotifyr} package and using the data to build k-nearest neighbor and decision tree models. \n\n## Data Preparation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(SPOTIFY_CLIENT_ID = SPOTIFY_CLIENT_ID)\nSys.setenv(SPOTIFY_CLIENT_SECRET = SPOTIFY_CLIENT_SECRET)\n\nauthorization_code <- get_spotify_authorization_code(scope = scopes()[c(1:19)]) #sets an authorization code that you'll need to provide for certain get_ functions via my_tracks <- get_my_saved_tracks(authorization = authorization_code)\n\naccess_token <- get_spotify_access_token() #takes ID and SECRET, sends to Spotify and receives an access token\n```\n:::\n\n\nUsing `get_my_saved_tracks()`, the Spotify API returns a dataframe of tracks and associated attributes. However, it will only return up to 50 tracks at a time, so I will need to make multiple requests. To do this, I will use a function to combine all the requests in one call.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffsets = seq(from = 0, to = 150, by = 50)\n\n#initializing an empty df\nmy_tracks <- data.frame(matrix(nrow = 0, ncol = 30))\n\n# function to get my 150 most recently liked tracks\nfor (i in seq_along(offsets)) {\n  liked_tracks = get_my_saved_tracks(authorization = authorization_code, limit = 50,\n                                     offset = offsets[i])\n  df_temp = as.data.frame(liked_tracks) # creating a temporary data frame\n  my_tracks <- rbind(my_tracks, df_temp) # binding the temporary data frame to my tracks data frame\n}\n```\n:::\n\n\nAdditionally, by giving the API a list of track IDs using get_track_audio_features(), I can get an audio features dataframe of all the tracks and some attributes of them.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naudio1 <- get_track_audio_features(my_tracks$track.id[1:100])\naudio2 <- get_track_audio_features(my_tracks$track.id[101:200])\n\naudio_features <- rbind(audio1, audio2)\n```\n:::\n\n\nThese track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks, so I need to append the 'track.name' column from my favorite tracks dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsam_audio <- my_tracks %>%\n  select(track.name) %>%\n  bind_cols(audio_features) %>%\n  mutate(name = \"sam\")\n```\n:::\n\n\nOne of my friends followed these same steps, and she sent me a .csv of her track audio features, which I will bind with my audio features dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboth_audio <- rbind(sam_audio, anna_audio)\n```\n:::\n\n\n\n## Data Exploration\n\nNow that the data is ready to go, let's do a little data exploration. There are a lot of cool audio features to explore. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(both_audio, aes(x = danceability, fill = name)) +\n  geom_density(alpha=0.5) +\n  scale_fill_manual(values=c(\"magenta4\", \"seagreen\"))+\n  labs(x=\"Danceability\", y=\"Density\", title = \"Distribution of Music Danceability Data\") +\n  guides(fill=guide_legend(title=\"Listener\")) +\n  theme_minimal()\n\n\nggplot(both_audio, aes(x = energy, fill = name)) +\n  geom_density(alpha=0.6) +\n  scale_fill_manual(values=c(\"magenta4\", \"seagreen\"))+\n  labs(x=\"Energy\", y=\"Density\", title = \"Distribution of Music Energy Data\") +\n  guides(fill=guide_legend(title=\"Listener\")) +\n  theme_minimal()\n\nrbind(anna_audio[which.max(anna_audio$danceability),],\n      sam_audio[which.max(sam_audio$danceability),]) %>%\n  select(track.name, danceability, name) %>%\n  gt::gt(caption = \"Liked tracks with the highest danceability\")\n```\n:::\n\n\n\n# Building Machine Learning Models\n\n## Modeling Prep\n\nFurther preparation is needed before running the models. First, I need to remove unnecessary columns including track urls and the track name. Next I'll split the data into training and testing sets, using a 75:25 split. Finally, I'll create my recipe, normalizing the nominal and numeric predictors and prep. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prepare data ----\nall_tracks_modeling <- both_audio %>%  \n  mutate_if(is.ordered, .funs = factor, ordered = F) %>%\n  select(-track.name, -type, -id, -uri, -track_href, -analysis_url) %>%\n  mutate(name = as.factor(name))\n\n# splitting the data ----\nset.seed(123)\n\ntracks_split <- initial_split(all_tracks_modeling, prop = 0.75)\ntracks_train <- training(tracks_split)\ntracks_test <- testing(tracks_split)\n\n# create a recipe ----\ntracks_recipe <- recipe(name ~ ., data = tracks_train) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>% #normalize numeric to make sure scale is okay\n  prep()\n```\n:::\n\n\n## K-Nearest Neighbor\n\nThe first model I want to build uses k-nearest neighbor. This is a classification problem (classifying the track as either belonging to my liked songs or my friends liked songs), so I will `set_mode()` to \"classification\" and `set_engine()` to \"kknn\".  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define our KNN model with tuning ----\nknn_spec_tune <- nearest_neighbor(neighbor = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\n# Check the model\n#knn_spec_tune\n```\n:::\n\n\nNext I need to define my workflow by adding the model and recipe I defined above. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a new workflow ----\nwf_knn_tune <- workflow() %>%\n  add_model(knn_spec_tune) %>%\n  add_recipe(tracks_recipe)\n```\n:::\n\n\nTo hopefully increase the accuracy of my model, I will use 10 fold cross validation to further split and train on my data. \n\n::: {.cell}\n\n```{.r .cell-code}\n# 10-fold CV on the training dataset ----\nset.seed(123)\n\ncv_folds <- tracks_train %>%\n  vfold_cv(v = 10)\n```\n:::\n\n\nNow that everything is ready to go, I can fit the workflow on the folds and check out the parameter tuning.  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the workflow on our predefined folds and hyperparameters ----\nfit_knn_cv <- wf_knn_tune %>%\n  tune_grid(resamples = cv_folds,\n            grid = 10)\n    \n# Check the performance with collect_metrics()\n#collect_metrics(fit_knn_cv)\n\n# plot cv results for parameter tuning ----\nautoplot(fit_knn_cv) + \n  theme_bw()\n```\n:::\n\n\nUsing the fit workflow, I can finalize and select the best iteration based on the ROC AUC and predict on the training and testing data. \n\n::: {.cell}\n\n```{.r .cell-code}\n# The final workflow for our KNN model ----\nfinal_knn_wf <- wf_knn_tune %>%\n  finalize_workflow(select_best(fit_knn_cv, metric = \"roc_auc\"))\n\ntrain_knn_fit <- fit(final_knn_wf, tracks_train)\n\ntrain_predict <- predict(object = train_knn_fit, new_data = tracks_train) %>% #predict the training set\n  bind_cols(tracks_train) #bind training set column to prediction\n\ntest_knn_predict <- predict(train_knn_fit, tracks_test) %>% #get prediction probabilities for test \n  bind_cols(tracks_test) %>%  #bind to testing column\n  mutate(name = as.factor(name))\n```\n:::\n\n\nWith my model now finalized, we can look at the accuracy of the training predictions compared to the testing predictions. \n\n::: {.cell}\n\n```{.r .cell-code}\n# report the accuracy for the training and testing ----\naccuracy(train_predict, truth = name, estimate = .pred_class) #get training accuracy\naccuracy(test_knn_predict, truth = name, estimate = .pred_class) #get accuracy of testing prediction\n# sensitivity(train_predict, truth = name, estimate = .pred_class)\n# specificity(train_predict, truth = name, estimate = .pred_class)\n# sensitivity(test_predict, truth = name, estimate = .pred_class)\n# specificity(test_predict, truth = name, estimate = .pred_class)\n```\n:::\n\n\nHere is the optimized workflow for the KNN model along with the result metrics. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Optimizing workflow ----\nfinal_knn_fit <- final_knn_wf %>% \n  last_fit(tracks_split)\n\n# last_fit() fit on the training data but then also evaluates on the testing data\nfinal_knn_result <- last_fit(final_knn_wf, tracks_split)\n\n# testing predictions and metrics ----\n#final_knn_result$.predictions \nknn_predict_data <- as.data.frame(final_knn_result$.predictions) %>%\n  bind_cols(tracks_test)\n\nfinal_knn_result$.metrics\n```\n:::\n\n\n### Decision Tree\n\nLet's build another model! This time I am going to be building a decision tree and tuning model hyperparameters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#new spec, tell the model that we are tuning hyperparams ----\ntree_spec_tune <- decision_tree(\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()\n) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\ntree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n#head(tree_grid)\n\n# setting up the workflow ----\nwf_tree_tune <- workflow() %>%\n  add_recipe(tracks_recipe) %>%\n  add_model(tree_spec_tune)\n\n#set up k-fold cv. This can be used for all the algorithms ----\ntracks_cv <- tracks_train %>%\n  vfold_cv(v=10)\n#tracks_cv\n\n# build trees ----\ndoParallel::registerDoParallel() #build trees in parallel\ntree_rs <- tune_grid(\n  wf_tree_tune,\n  resamples = tracks_cv,\n  grid = tree_grid,\n  metrics = metric_set(accuracy)\n)\n\n# Use autoplot() to examine how different parameter configurations relate to accuracy ----\nautoplot(tree_rs) +\n  theme_bw()\n\n# select hyperparameter ----\n#show_best(tree_rs) # showing\nselect_best(tree_rs) # what we'll input\n\n# finalize the model specification where we have replaced the tune functions with optimized values ----\nfinal_tree <- finalize_workflow(wf_tree_tune, select_best(tree_rs))\n#final_tree\n\n# final fitting ----\nfinal_tree_fit <- fit(final_tree, data = tracks_train) \n\n# last_fit() fit on the training data but then also evaluates on the testing data\nfinal_tree_result <- last_fit(final_tree, tracks_split)\n\n# testing predictions and metrics ----\n#final_tree_result$.predictions \nfinal_tree_result$.metrics\n\npredict_tree_data <- predict(final_tree_fit, tracks_test) %>% #get prediction probabilities for test \n  bind_cols(tracks_test) %>%  #bind to testing column\n  mutate(name = as.factor(name))\n\n#Visualize variable importance ----\nfinal_tree_fit %>%\n  vip(geom = \"point\") +\n  theme_bw()\n\n# differences between groups\n# ggplot(predict_tree_data, aes(.pred_class, duration_ms)) +\n#   geom_boxplot()\n```\n:::\n\n\n\n## Bagged Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up the bagged tree model ----\nbagged_tree <- bag_tree(\n  cost_complexity = tune(),\n  min_n = tune()\n) %>%\n  set_engine(\"rpart\", times = 50) %>%\n  set_mode(\"classification\")\n\n# workflow ----\nwf_bag_tune <- workflow() %>%\n  add_recipe(tracks_recipe) %>%\n  add_model(bagged_tree)\n\n# set up the tuning ----\nbag_grid <- grid_regular(cost_complexity(), min_n(), levels = 5)\n\nbag_rs <- tune_grid(\n  wf_bag_tune,\n  resamples = tracks_cv,\n  grid = bag_grid,\n  metrics = metric_set(accuracy))\n\n\n# select hyperparameter ----\n#show_best(bag_rs) # showing\nselect_best(bag_rs) # what we'll input\n\n# finalize the model specification ----\nfinal_bag <- finalize_workflow(wf_bag_tune, select_best(bag_rs))\n#final_bag\n\n# final fitting ----\nfinal_bag_fit <- fit(final_bag, data = tracks_train) \n\n# last_fit() fit on the training data but then also evaluates on the testing data\nfinal_bag_result <- last_fit(final_bag, tracks_split)\n\n# testing predictions and metrics ----\nbag_data <- predict(final_bag_fit, tracks_test) %>% #get prediction probabilities for test \n  bind_cols(tracks_test) %>%  #bind to testing column\n  mutate(name = as.factor(name))\n\n#final_bag_result$.predictions \n\n# report final metrics ----\nfinal_bag_result$.metrics\n```\n:::\n\n\n## Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# random forest ----\nrf_model <- rand_forest(mtry = tune(),\n                  trees = tune()) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# workflow ----\nrf_workflow <- workflow() %>%\n  add_model(rf_model) %>%\n  add_recipe(tracks_recipe)\n\n# parameter tuning ----\nrf_cv_tune <- rf_workflow %>%\n  tune_grid(resamples = cv_folds, grid = 10) #use cross validation to tune mtry and trees parameters\n\n#get metrics from tuning cv to pick best model ----\n#collect_metrics(rf_cv_tune) \n\n#plot cv results for parameter tuning ----\nautoplot(rf_cv_tune) + \n  theme_bw()\n\n# finalize workflow ----\nrf_best <- show_best(rf_cv_tune, n = 1, metric = \"roc_auc\") #get metrics for best random forest model\nrf_best\n\nrf_final <- finalize_workflow(rf_workflow,\n                              select_best(rf_cv_tune, metric = \"roc_auc\"))\n\n# model fitting ----\ntrain_fit_rf <- fit(rf_final, tracks_train) #fit the KNN model to the training set\n#train_fit_rf\n\n# prediction probabilities ----\ntest_predict_rf <- predict(train_fit_rf, tracks_test) %>% #get prediction probabilities for test \n  bind_cols(tracks_test) %>%  #bind to testing column\n  mutate(name = as.factor(name))\n\ntest_predict2_rf <- predict(train_fit_rf, tracks_test, type = \"prob\") %>% #get testing prediction\n  bind_cols(tracks_test) %>%  #bind to testing column\n  mutate(name = as.factor(name))\n\n# model metrics and evaluation\n#accuracy(test_predict_rf, truth = name, estimate = .pred_class) #get accuracy of testing prediction\n\nrf_final_result <- last_fit(rf_final, tracks_split)\n\n# testing predictions and metrics ----\n#rf_final_result$.predictions \nrf_predict_data <- as.data.frame(rf_final_result$.predictions) %>%\n  bind_cols(tracks_test)\n\nrf_final_result$.metrics\n\n# roc auc curve\ntest_roc_auc_rf <- roc_curve(test_predict2_rf, name, .pred_anna)\n```\n:::\n\n\n# Comparison of model performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# store the final result metrics\nknn_metrics <- final_knn_result$.metrics[[1]]\ntree_metrics <- final_tree_result$.metrics[[1]]\nbag_metrics <- final_bag_result$.metrics[[1]]\nrf_metrics <- rf_final_result$.metrics[[1]]\n\n# combine tmetrics into df for table\ncomparison_data <- data.frame(\n  Model = c(\"KNN\", \"Decision Tree\", \"Bagged Tree\", \"Random Forest\"),\n  Accuracy = c(knn_metrics$.estimate[1], tree_metrics$.estimate[1], bag_metrics$.estimate[1], rf_metrics$.estimate[1]),\n  ROC_AUC = c(knn_metrics$.estimate[2], tree_metrics$.estimate[2], bag_metrics$.estimate[2], rf_metrics$.estimate[2])\n)\n\n# make table\ncomparison_data %>%\n  gt() %>%\n  tab_header(\n    title = \"Model Comparison\",\n    subtitle = \"Accuracy and ROC AUC on Testing Data\"\n  ) %>%\n  fmt_number(\n    columns = c(Accuracy, ROC_AUC),\n    decimals = 3\n  )\n\n# pivot data for plottig\ncomparison_data_long <- comparison_data %>%\n  pivot_longer(cols = c(Accuracy, ROC_AUC), names_to = \"Metric\", values_to = \"Value\")\n\n# plotting\nggplot(comparison_data_long, aes(x = Model, y = Value, fill = Metric)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.7, color = \"black\") +\n  labs(title = \"Model Comparison\",\n       y = \"Metric Value\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"lightblue\", \"seagreen\"))\n\n# confusion matrix\nknn_conf <- test_knn_predict %>%\n  conf_mat(truth = name, estimate = .pred_class) %>%\n  autoplot(type = 'heatmap') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(title = \"KNN\")\nrf_conf <- test_predict_rf %>% \n  conf_mat(truth = name, estimate = .pred_class) %>% #create confusion matrix\n  autoplot(type = \"heatmap\") + #plot confusion matrix with heatmap\n  theme_bw() + #change theme\n  theme(axis.text.x = element_text(angle = 30, hjust=1)) +\n  #rotate axis labels\n  labs(title = \"Random Forest\")\nbag_conf <- bag_data %>%\n  conf_mat(truth = name, estimate = .pred_class) %>%\n  autoplot(type = 'heatmap') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(title = \"Bagged Tree\")\ntree_conf <- predict_tree_data %>%\n  conf_mat(truth = name, estimate = .pred_class) %>%\n  autoplot(type = 'heatmap') +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +\n  labs(title = \"Decision Tree\")\n\nbag_conf + knn_conf + rf_conf + tree_conf\n```\n:::\n\n\n# Thoughts and Conclusions\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}